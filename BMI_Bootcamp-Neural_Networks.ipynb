{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BMI ML Bootcamp #3 - Basic Neural Net Implementation with MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted pretty heavily from BMI 219 HW 1 - shout out to Kangway\n",
    "\n",
    "PyTorch tutorial - https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html\n",
    "\n",
    "The pytorch documentation can be a bit rough. If you're having trouble, feel free to ask me (or Scott or Erin of course :) ) - this tutorial might be difficult if you're unfamiliar with NNs/pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, we'll be using the MNIST dataset, consisting of handwritted numbers and their accompanying label. The first thing to do is download, load, and normalize this dataset. Running the cell below will create a 'data/MNIST' file containing the data. We also create a preprocessing transform to normalize the mean and standard deviation of training set to be 0.1307 and 0.3081 respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    './data', train=True, download=True,\n",
    "    transform=preprocessing)\n",
    "                               \n",
    "test_dataset = datasets.MNIST(\n",
    "    './data',train=False, download=True, \n",
    "    transform=preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many training examples are there? How many examples in the test set? What are the image dimensions? Where can you find the corresponding labels for each training example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the training data further into training and validation sets - this well help us to prevent overfitting. What split did you choose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set = torch.utils.data.random_split(train_dataset, ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell plots a few random training examples along with the labels, always good to sanity check your data\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20,5))\n",
    "\n",
    "for ax_index, i in enumerate(np.random.choice(list(range(train_set.dataset.data.shape[0])), 4)):\n",
    "    x, _ = train_set.dataset[i] # x is now a torch.Tensor\n",
    "    axes[ax_index].imshow(x.numpy()[0], cmap='gray')\n",
    "    axes[ax_index].set_title(train_set.dataset.targets[i].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up dataloaders - one for training, one for validation, and one for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128  ### Please change this as necessary\n",
    "NUM_WORKERS = 1  ### Use more workers for more CPU threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_set,\n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS)\n",
    "\n",
    "# Set up test_loader here, same as the training loader\n",
    "test_loader = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the neural network architecture\n",
    "\n",
    "I implemented SmallNet with 2 2dconv/max pool layers, followed by a few fully connected layers, but you can do this however you would like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 1, 3) # 1 input image channel, 1 output channels, 3x3 square convolution\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(1, 1, 3) # 1 input image channel, 1 output channels, 3x3 square convolution\n",
    "        self.fc1 = nn.Linear(1*5*5, 15)\n",
    "        self.fc2 = nn.Linear(15, 10) #10 possibilities (0 - 9), so we want 10 output nodes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1,1*5*5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1) # Mutually exclusive classification task\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make your own architecture here - go ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YourNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(YourNet, self).__init__()\n",
    "\n",
    "        #add your layers here\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Add your forward pass here - choose whatever activation functions you think are best\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define your training function here\n",
    "\n",
    "Your training function should accomplish a few things, namely\n",
    "* iterate through n epochs\n",
    "* load batches\n",
    "* feed forward\n",
    "* calculate loss\n",
    "* backpropagation\n",
    "* return the losses\n",
    "\n",
    "I've included my training function header and skeleton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, val_loader, model, optimizer, criterion, device,\n",
    "          n_epochs=10, log_interval = 1, save = False, prefix = 'test', **kwargs):\n",
    "    \n",
    "    model.train()\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(n_epochs + 1):\n",
    "        #training loss\n",
    "        epoch_losses = []\n",
    "        val_epoch_losses = []\n",
    "        for batch_idx, examples in enumerate(train_loader):\n",
    "            # Get values and lables from train_loader, send to device\n",
    "            values, labels = examples \n",
    "            values = values.to(device, dtype=torch.float) #train on set device (I used GPU) \n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad() #zero gradients\n",
    "\n",
    "            ? #feed forward\n",
    "            ? #get loss\n",
    "            ? #back propagation\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_losses.append(loss.item())\n",
    "        \n",
    "        #validation loss - don't update gradients\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, examples in enumerate(val_loader):\n",
    "                ? # Get values and labels from val_loader, send to device\n",
    "                \n",
    "                ? #feed forward\n",
    "                ? #get loss \n",
    "                \n",
    "                val_epoch_losses.append(loss.item())\n",
    "            \n",
    "        ? # Update losses and val_losses\n",
    "    \n",
    "        if epoch % log_interval == 0:\n",
    "            print('Train Epoch: {} \\tLoss: {:.6f}, Val: {:.6f}'.format(\n",
    "                epoch, losses[epoch], val_losses[epoch]))\n",
    "        \n",
    "            #save temporary model\n",
    "            if save:\n",
    "                state = {'epoch': epoch, 'state_dict': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict()}\n",
    "                torch.save(state, f'{prefix}-{epoch}.pth')\n",
    "\n",
    "    return losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if cuda is available - if you're running this on your laptop, it probably won't be\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call your training function and record the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# These are the hyper-parameters that I used, but feel free to mess around\n",
    "lr = 0.001 \n",
    "n_epochs = 50\n",
    "log_interval = n_epochs / 10\n",
    "\n",
    "model = SmallNet() # Change to your model, once you've decided on an architecture\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr) # Don't have to use Adam - checkout SGD etc.\n",
    "criterion = nn.CrossEntropyLoss() # Why are we using cross entropy loss here? Could we be using a different loss function?\n",
    "\n",
    "losses = train(train_loader, val_loader, model, optimizer, criterion, device, \n",
    "               n_epochs = n_epochs, log_interval = log_interval, \n",
    "               save = False, prefix = f'SmallNet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the validation and loss curves\n",
    "\n",
    "Plotting the loss curves helps us to see if our model is still training, and if the loss at each epoch looks reasonable. By including a validation set, we are able to observe if our model is overfitting, indicated by a validation loss much higher than the training loss. When you plot your training and validation losses, do you observe anything that looks like overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses[0], label = \"Training\")\n",
    "plt.plot(losses[1], label = \"Validation\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the performance of the model using the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images = images.to(device, dtype=torch.float)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model.forward(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels)\n",
    "        for i in range(c.shape[0]):\n",
    "            label = labels[i].item()\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "correct = []\n",
    "for i in range(10):\n",
    "    correct.append(class_correct[i] / class_total[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize a few test examples and their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(20,5))\n",
    "\n",
    "mini_batch, labels = next(iter(test_loader))\n",
    "with torch.no_grad():\n",
    "    outputs = model.forward(mini_batch)\n",
    "for ax_index, i in enumerate(zip(mini_batch[:4], labels[:4], outputs[:4])):\n",
    "    axes[ax_index].imshow(i[0].numpy()[0], cmap='gray')\n",
    "    axes[ax_index].set_title(f'Predicted: {torch.argmax(i[2]).item()}, Actual: {i[1].item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "plt.bar(np.arange(10), correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Train an autoencoder, either on the MNIST dataset or the CelebA Faces dataset\n",
    "\n",
    "Unlike the classification problem above, the goal of an autoencoder is to compress input data to a lower dimensional representation (encoding layers), followed by decompressing to recreate the initial input (decoding layers). This means that the dimensions of the output layer will need to change, as well as the loss function. For example, a typical architecture for MNIST would be [784, 392, 196, 98, 2, 98, 196, 392, 784] (28 * 28 * 1 = 784, for the input size). How would you modify SmallNet or your classifier architecture to approach this problem?\n",
    "\n",
    "If you're interested in doing this, read https://www.cs.toronto.edu/~hinton/science.pdf \"Reducing the dimensionality of data with neural networks\". They discuss their approach and architecture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ml-tutorial)",
   "language": "python",
   "name": "ml-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
